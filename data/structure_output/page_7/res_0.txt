{"type": "text", "bbox": [775, 391, 1331, 787], "res": [{"text": "become more amplified with deteriorating", "confidence": 0.9997867345809937, "text_region": [[777.0, 398.0], [1304.0, 400.0], [1304.0, 425.0], [777.0, 423.0]]}, {"text": "data quality. Another reason why an LLM", "confidence": 0.9842677712440491, "text_region": [[780.0, 435.0], [1278.0, 435.0], [1278.0, 456.0], [780.0, 456.0]]}, {"text": "may output erroneous results is due to the", "confidence": 0.9826922416687012, "text_region": [[780.0, 467.0], [1303.0, 467.0], [1303.0, 488.0], [780.0, 488.0]]}, {"text": "Iack of context in a user's prompt. Without", "confidence": 0.9859243631362915, "text_region": [[779.0, 498.0], [1298.0, 499.0], [1298.0, 521.0], [779.0, 520.0]]}, {"text": "proper context, the LLM doesn't \"actually", "confidence": 0.9780905842781067, "text_region": [[779.0, 531.0], [1276.0, 531.0], [1276.0, 555.0], [779.0, 555.0]]}, {"text": "know\" what you expect from it. For example,", "confidence": 0.994714617729187, "text_region": [[779.0, 564.0], [1319.0, 564.0], [1319.0, 588.0], [779.0, 588.0]]}, {"text": "f you prompt \"What's the capital?\" and", "confidence": 0.9865153431892395, "text_region": [[779.0, 596.0], [1266.0, 596.0], [1266.0, 620.0], [779.0, 620.0]]}, {"text": "do not specify the country, then the model", "confidence": 0.9957863688468933, "text_region": [[778.0, 629.0], [1305.0, 629.0], [1305.0, 653.0], [778.0, 653.0]]}, {"text": "has no way of knowing what you're looking", "confidence": 0.9887944459915161, "text_region": [[777.0, 660.0], [1308.0, 661.0], [1307.0, 686.0], [777.0, 685.0]]}, {"text": "for. Without context, the LLM is bound to", "confidence": 0.985406756401062, "text_region": [[779.0, 695.0], [1265.0, 695.0], [1265.0, 716.0], [779.0, 716.0]]}, {"text": "generate results that won't align with what", "confidence": 0.9998270869255066, "text_region": [[777.0, 725.0], [1308.0, 724.0], [1309.0, 749.0], [777.0, 750.0]]}, {"text": "you're looking for. (See Fig 1.1).", "confidence": 0.9875074028968811, "text_region": [[777.0, 757.0], [1136.0, 756.0], [1136.0, 781.0], [777.0, 782.0]]}], "img_idx": 0}
{"type": "text", "bbox": [152, 392, 721, 819], "res": [{"text": "Firstly, you'll see how the word.", "confidence": 0.990166962146759, "text_region": [[155.0, 399.0], [526.0, 398.0], [526.0, 423.0], [155.0, 424.0]]}, {"text": "'hallucinations\" is practically everywhere", "confidence": 0.9871670007705688, "text_region": [[155.0, 431.0], [663.0, 433.0], [663.0, 458.0], [155.0, 456.0]]}, {"text": "there's a mention of LLMs. Also better termed", "confidence": 0.9664127826690674, "text_region": [[155.0, 465.0], [709.0, 466.0], [709.0, 486.0], [155.0, 485.0]]}, {"text": "as \"confabulations\". This is the model.", "confidence": 0.973713219165802, "text_region": [[156.0, 499.0], [617.0, 499.0], [617.0, 519.0], [156.0, 519.0]]}, {"text": "throwing plausible but incorrect or entirely", "confidence": 0.9887906908988953, "text_region": [[154.0, 528.0], [680.0, 531.0], [679.0, 555.0], [154.0, 553.0]]}, {"text": "fabricated answers at you, meaning you", "confidence": 0.9917753338813782, "text_region": [[154.0, 562.0], [658.0, 564.0], [658.0, 588.0], [154.0, 586.0]]}, {"text": "should always double-check what the", "confidence": 0.9914736151695251, "text_region": [[155.0, 596.0], [629.0, 595.0], [629.0, 617.0], [155.0, 618.0]]}, {"text": "LLM outputs. There are, of course, several", "confidence": 0.9911122918128967, "text_region": [[156.0, 630.0], [658.0, 630.0], [658.0, 651.0], [156.0, 651.0]]}, {"text": "reasons why this happens. Primarily, LLMs", "confidence": 0.9995149970054626, "text_region": [[155.0, 661.0], [667.0, 661.0], [667.0, 685.0], [155.0, 685.0]]}, {"text": "Iack \"common sense,\" i.e., they're not.", "confidence": 0.9717741012573242, "text_region": [[155.0, 694.0], [613.0, 695.0], [613.0, 716.0], [155.0, 715.0]]}, {"text": "primarily reasoning machines.Remember", "confidence": 0.9848691821098328, "text_region": [[156.0, 727.0], [678.0, 726.0], [678.0, 747.0], [156.0, 748.0]]}, {"text": "that they're trained to predict the word that's", "confidence": 0.9982510209083557, "text_region": [[155.0, 759.0], [710.0, 758.0], [710.0, 780.0], [155.0, 781.0]]}, {"text": "most likely to occur next. The problem may", "confidence": 0.9778175354003906, "text_region": [[156.0, 790.0], [685.0, 792.0], [685.0, 814.0], [156.0, 812.0]]}], "img_idx": 0}
{"type": "figure", "bbox": [12, 429, 1415, 2104], "res": [{"text": "\"hallucinations\" is practically everywhere", "confidence": 0.9992657899856567, "text_region": [[152.0, 431.0], [664.0, 433.0], [664.0, 457.0], [152.0, 455.0]]}, {"text": "data quality. Another reason why an LLM", "confidence": 0.9971728324890137, "text_region": [[777.0, 434.0], [1278.0, 434.0], [1278.0, 457.0], [777.0, 457.0]]}, {"text": "there's a mention of LLMs. Also better termed.", "confidence": 0.9920552372932434, "text_region": [[151.0, 462.0], [710.0, 464.0], [710.0, 488.0], [151.0, 487.0]]}, {"text": "may output erroneous results is due to the", "confidence": 0.989331841468811, "text_region": [[775.0, 466.0], [1305.0, 466.0], [1305.0, 490.0], [775.0, 490.0]]}, {"text": "as \"confabulations\". This is the model", "confidence": 0.9990337491035461, "text_region": [[151.0, 497.0], [619.0, 495.0], [619.0, 520.0], [151.0, 522.0]]}, {"text": "lack of context in a user's prompt. Without", "confidence": 0.9811716079711914, "text_region": [[771.0, 495.0], [1299.0, 494.0], [1299.0, 523.0], [771.0, 525.0]]}, {"text": "throwing plausible but incorrect or entirely", "confidence": 0.9990732073783875, "text_region": [[152.0, 530.0], [678.0, 530.0], [678.0, 555.0], [152.0, 555.0]]}, {"text": "proper context, the LLM doesn't \"actually", "confidence": 0.9913824200630188, "text_region": [[777.0, 530.0], [1276.0, 530.0], [1276.0, 555.0], [777.0, 555.0]]}, {"text": "fabricated answers at you, meaning you", "confidence": 0.9995231628417969, "text_region": [[149.0, 558.0], [659.0, 562.0], [659.0, 591.0], [149.0, 588.0]]}, {"text": "know\" what you expect from it. For example.", "confidence": 0.9852805733680725, "text_region": [[777.0, 563.0], [1319.0, 563.0], [1319.0, 588.0], [777.0, 588.0]]}, {"text": "should always double-check what the", "confidence": 0.9976293444633484, "text_region": [[152.0, 597.0], [629.0, 597.0], [629.0, 619.0], [152.0, 619.0]]}, {"text": "if you prompt \"what's the capital?\" and", "confidence": 0.9837852716445923, "text_region": [[777.0, 595.0], [1264.0, 595.0], [1264.0, 619.0], [777.0, 619.0]]}, {"text": "LLM outputs. There are, of course, several", "confidence": 0.9888116717338562, "text_region": [[152.0, 628.0], [659.0, 628.0], [659.0, 652.0], [152.0, 652.0]]}, {"text": "do not specify the country, then the model", "confidence": 0.9870624542236328, "text_region": [[777.0, 628.0], [1305.0, 628.0], [1305.0, 652.0], [777.0, 652.0]]}, {"text": "reasons why this happens. Primarily, LLMs", "confidence": 0.9864278435707092, "text_region": [[151.0, 659.0], [664.0, 659.0], [664.0, 684.0], [151.0, 684.0]]}, {"text": "has no way of knowing what you're looking", "confidence": 0.9933373332023621, "text_region": [[775.0, 661.0], [1308.0, 661.0], [1308.0, 685.0], [775.0, 685.0]]}, {"text": "Iack \"common sense,\" i.e., they're not", "confidence": 0.9804831147193909, "text_region": [[152.0, 691.0], [614.0, 691.0], [614.0, 715.0], [152.0, 715.0]]}, {"text": "for. Without context, the LLM is bound to", "confidence": 0.9750391244888306, "text_region": [[775.0, 692.0], [1264.0, 692.0], [1264.0, 717.0], [775.0, 717.0]]}, {"text": "primarily reasoning machines. Remember", "confidence": 0.9998039603233337, "text_region": [[154.0, 726.0], [678.0, 726.0], [678.0, 750.0], [154.0, 750.0]]}, {"text": "generate results that won't align with what.", "confidence": 0.9949805736541748, "text_region": [[773.0, 724.0], [1310.0, 720.0], [1310.0, 750.0], [773.0, 754.0]]}, {"text": "that they're trained to predict the word that's.", "confidence": 0.9888591170310974, "text_region": [[152.0, 759.0], [712.0, 759.0], [712.0, 783.0], [152.0, 783.0]]}, {"text": "you're looking for. (See Fig 1.1)", "confidence": 0.9963815808296204, "text_region": [[773.0, 755.0], [1140.0, 755.0], [1140.0, 785.0], [773.0, 785.0]]}, {"text": "most likely to occur next. The problem may.", "confidence": 0.9836171865463257, "text_region": [[151.0, 787.0], [687.0, 788.0], [687.0, 818.0], [151.0, 816.0]]}, {"text": "23:56", "confidence": 0.997724711894989, "text_region": [[826.0, 987.0], [870.0, 987.0], [870.0, 1014.0], [826.0, 1014.0]]}, {"text": "", "confidence": 0.0, "text_region": [[1043.0, 987.0], [1127.0, 987.0], [1127.0, 1012.0], [1043.0, 1012.0]]}, {"text": "User Input", "confidence": 0.9953103065490723, "text_region": [[912.0, 1083.0], [984.0, 1087.0], [983.0, 1106.0], [911.0, 1102.0]]}, {"text": "HI THERE!", "confidence": 0.9828470945358276, "text_region": [[450.0, 1096.0], [587.0, 1096.0], [587.0, 1136.0], [450.0, 1136.0]]}, {"text": "Can you", "confidence": 0.989838719367981, "text_region": [[915.0, 1108.0], [998.0, 1108.0], [998.0, 1127.0], [915.0, 1127.0]]}, {"text": "recommend a", "confidence": 0.9894174337387085, "text_region": [[913.0, 1129.0], [1059.0, 1129.0], [1059.0, 1151.0], [913.0, 1151.0]]}, {"text": "delicious recipe", "confidence": 0.9947915077209473, "text_region": [[912.0, 1149.0], [1070.0, 1153.0], [1069.0, 1176.0], [911.0, 1172.0]]}, {"text": "for dinner?", "confidence": 0.9703830480575562, "text_region": [[913.0, 1178.0], [1020.0, 1178.0], [1020.0, 1197.0], [913.0, 1197.0]]}, {"text": "LLM Response", "confidence": 0.9257643222808838, "text_region": [[845.0, 1263.0], [940.0, 1263.0], [940.0, 1282.0], [845.0, 1282.0]]}, {"text": "Yes, here is a", "confidence": 0.9872341752052307, "text_region": [[850.0, 1289.0], [978.0, 1289.0], [978.0, 1308.0], [850.0, 1308.0]]}, {"text": "delicious recipe for", "confidence": 0.9980825185775757, "text_region": [[850.0, 1312.0], [1034.0, 1312.0], [1034.0, 1335.0], [850.0, 1335.0]]}, {"text": "lunch. So how", "confidence": 0.9633297324180603, "text_region": [[850.0, 1334.0], [985.0, 1336.0], [985.0, 1356.0], [850.0, 1354.0]]}, {"text": "about fried", "confidence": 0.9594370126724243, "text_region": [[852.0, 1361.0], [961.0, 1361.0], [961.0, 1380.0], [852.0, 1380.0]]}, {"text": "chicken with", "confidence": 0.9812278747558594, "text_region": [[850.0, 1383.0], [973.0, 1383.0], [973.0, 1403.0], [850.0, 1403.0]]}, {"text": "mashed potatoes?", "confidence": 0.9978935122489929, "text_region": [[849.0, 1406.0], [1036.0, 1406.0], [1036.0, 1429.0], [849.0, 1429.0]]}, {"text": "In addition,", "confidence": 0.9701046347618103, "text_region": [[850.0, 1431.0], [959.0, 1431.0], [959.0, 1450.0], [850.0, 1450.0]]}, {"text": "tomatoes are also", "confidence": 0.971239447593689, "text_region": [[850.0, 1453.0], [1029.0, 1453.0], [1029.0, 1471.0], [850.0, 1471.0]]}, {"text": "an excellent", "confidence": 0.9628067016601562, "text_region": [[852.0, 1478.0], [970.0, 1478.0], [970.0, 1497.0], [852.0, 1497.0]]}, {"text": "pairing for this", "confidence": 0.9997807741165161, "text_region": [[849.0, 1500.0], [994.0, 1500.0], [994.0, 1523.0], [849.0, 1523.0]]}, {"text": "000", "confidence": 0.957707941532135, "text_region": [[578.0, 1516.0], [626.0, 1516.0], [626.0, 1535.0], [578.0, 1535.0]]}, {"text": "dish as they are", "confidence": 0.9988924264907837, "text_region": [[849.0, 1523.0], [1008.0, 1523.0], [1008.0, 1546.0], [849.0, 1546.0]]}, {"text": "rich in calcium.", "confidence": 0.9715620875358582, "text_region": [[849.0, 1546.0], [1001.0, 1546.0], [1001.0, 1568.0], [849.0, 1568.0]]}, {"text": "Enjoy this steak!", "confidence": 0.9995562434196472, "text_region": [[849.0, 1570.0], [1006.0, 1570.0], [1006.0, 1593.0], [849.0, 1593.0]]}, {"text": "Hallucination Explanation", "confidence": 0.9997190833091736, "text_region": [[577.0, 1732.0], [910.0, 1732.0], [910.0, 1757.0], [577.0, 1757.0]]}, {"text": "Input-Conflicting Hallucination: : Context-Conflicting Hallucination: : Fact-Conflicting Hallucination:", "confidence": 0.9754900336265564, "text_region": [[180.0, 1783.0], [1317.0, 1783.0], [1317.0, 1807.0], [180.0, 1807.0]]}, {"text": "the user wants a recipe for", "confidence": 0.9996190071105957, "text_region": [[179.0, 1813.0], [482.0, 1813.0], [482.0, 1835.0], [179.0, 1835.0]]}, {"text": "steak has not been mentioned in", "confidence": 0.9998117685317993, "text_region": [[561.0, 1813.0], [940.0, 1813.0], [940.0, 1835.0], [561.0, 1835.0]]}, {"text": ": tomatoes are not rich in", "confidence": 0.9930508732795715, "text_region": [[954.0, 1813.0], [1247.0, 1813.0], [1247.0, 1835.0], [954.0, 1835.0]]}, {"text": "dinner while LLM provide one", "confidence": 0.9945011734962463, "text_region": [[179.0, 1837.0], [503.0, 1841.0], [503.0, 1863.0], [179.0, 1860.0]]}, {"text": "the preceding context.", "confidence": 0.9995603561401367, "text_region": [[561.0, 1841.0], [815.0, 1841.0], [815.0, 1863.0], [561.0, 1863.0]]}, {"text": "calcium in fact.", "confidence": 0.9932486414909363, "text_region": [[970.0, 1837.0], [1149.0, 1839.0], [1148.0, 1863.0], [969.0, 1861.0]]}, {"text": "for lunch.", "confidence": 0.9974225163459778, "text_region": [[179.0, 1867.0], [286.0, 1867.0], [286.0, 1891.0], [179.0, 1891.0]]}, {"text": "Fig 1.1: Example of LLM hallucination on ChatGPT.", "confidence": 0.9747456908226013, "text_region": [[473.0, 1930.0], [1013.0, 1928.0], [1013.0, 1952.0], [473.0, 1954.0]]}, {"text": "Galileo", "confidence": 0.9797354340553284, "text_region": [[245.0, 1990.0], [354.0, 1995.0], [353.0, 2033.0], [243.0, 2029.0]]}, {"text": "www.rungalileo.io", "confidence": 0.9991854429244995, "text_region": [[1147.0, 2005.0], [1336.0, 2005.0], [1336.0, 2027.0], [1147.0, 2027.0]]}], "img_idx": 0}
