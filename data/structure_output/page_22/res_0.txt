{"type": "text", "bbox": [152, 184, 1291, 286], "res": [{"text": "If you look at Fig 3.1, you'll see how guiding the model through a series of logical operations", "confidence": 0.9895061254501343, "text_region": [[157.0, 194.0], [1278.0, 194.0], [1278.0, 218.0], [157.0, 218.0]]}, {"text": "or steps helps it better understand how to approach each of the queries, compared to a.", "confidence": 0.9883872866630554, "text_region": [[156.0, 226.0], [1253.0, 226.0], [1253.0, 251.0], [156.0, 251.0]]}, {"text": "standard prompt, which fails to give the correct response..", "confidence": 0.9881487488746643, "text_region": [[156.0, 257.0], [872.0, 259.0], [872.0, 282.0], [156.0, 280.0]]}], "img_idx": 0}
{"type": "text", "bbox": [153, 313, 1335, 482], "res": [{"text": "While chain of thought prompting is a great technique for enhancing the model's reasoning,", "confidence": 0.9960669875144958, "text_region": [[155.0, 321.0], [1302.0, 322.0], [1302.0, 349.0], [155.0, 347.0]]}, {"text": "designing the prompt itself can be quite challenging. You'll also notice how the results depend", "confidence": 0.9888203144073486, "text_region": [[158.0, 358.0], [1326.0, 358.0], [1326.0, 380.0], [158.0, 380.0]]}, {"text": "on the results of intermediary steps (or the thought process that the model follows), so if", "confidence": 0.9919164180755615, "text_region": [[157.0, 387.0], [1256.0, 386.0], [1256.0, 413.0], [157.0, 415.0]]}, {"text": "any of the steps are flawed, you'll end up with an incorrect response. Think \"error propagates", "confidence": 0.9947837591171265, "text_region": [[155.0, 419.0], [1308.0, 421.0], [1308.0, 448.0], [155.0, 445.0]]}, {"text": "through the chain.\"", "confidence": 0.9431451559066772, "text_region": [[155.0, 453.0], [392.0, 453.0], [392.0, 475.0], [155.0, 475.0]]}], "img_idx": 0}
{"type": "text", "bbox": [152, 1369, 592, 1439], "res": [{"text": "The idea behind ThoT prompting is", "confidence": 0.9996581673622131, "text_region": [[154.0, 1376.0], [586.0, 1378.0], [586.0, 1404.0], [154.0, 1402.0]]}, {"text": "instructing the model to:.", "confidence": 0.9890577793121338, "text_region": [[154.0, 1411.0], [458.0, 1413.0], [458.0, 1435.0], [154.0, 1432.0]]}], "img_idx": 0}
{"type": "figure", "bbox": [105, 479, 1479, 2105], "res": [{"text": "THREAD OF THOUGHT (THOT)", "confidence": 0.8012259602546692, "text_region": [[157.0, 540.0], [941.0, 540.0], [941.0, 592.0], [157.0, 592.0]]}, {"text": "The Thread of Thought (ThoT)", "confidence": 0.9788488149642944, "text_region": [[153.0, 660.0], [521.0, 660.0], [521.0, 689.0], [153.0, 689.0]]}, {"text": "prompting technique is very", "confidence": 0.9940803050994873, "text_region": [[155.0, 696.0], [503.0, 696.0], [503.0, 720.0], [155.0, 720.0]]}, {"text": "intuitive and works very well when", "confidence": 0.9889690279960632, "text_region": [[151.0, 725.0], [577.0, 726.0], [577.0, 755.0], [151.0, 753.0]]}, {"text": "the retrieved information (or", "confidence": 0.9701652526855469, "text_region": [[155.0, 762.0], [509.0, 762.0], [509.0, 784.0], [155.0, 784.0]]}, {"text": "context) is \"chaotic.\" What does", "confidence": 0.990641713142395, "text_region": [[155.0, 794.0], [550.0, 794.0], [550.0, 818.0], [155.0, 818.0]]}, {"text": "this mean? A chaotic context is", "confidence": 0.9924739003181458, "text_region": [[153.0, 828.0], [543.0, 828.0], [543.0, 850.0], [153.0, 850.0]]}, {"text": "something that is often full of", "confidence": 0.9831869006156921, "text_region": [[153.0, 860.0], [519.0, 857.0], [519.0, 880.0], [153.0, 884.0]]}, {"text": "unrelated, complex information", "confidence": 0.999786913394928, "text_region": [[155.0, 892.0], [546.0, 892.0], [546.0, 916.0], [155.0, 916.0]]}, {"text": "that will most likely not contribute", "confidence": 0.9989537000656128, "text_region": [[155.0, 924.0], [570.0, 924.0], [570.0, 948.0], [155.0, 948.0]]}, {"text": "anything to the model's response.", "confidence": 0.9923708438873291, "text_region": [[157.0, 958.0], [574.0, 958.0], [574.0, 982.0], [157.0, 982.0]]}, {"text": "It is also characterized by a lack", "confidence": 0.9897187352180481, "text_region": [[153.0, 989.0], [553.0, 989.0], [553.0, 1013.0], [153.0, 1013.0]]}, {"text": "of coherency and a muddle of", "confidence": 0.9992863535881042, "text_region": [[151.0, 1019.0], [536.0, 1018.0], [536.0, 1046.0], [151.0, 1048.0]]}, {"text": "details. The onus is on the model", "confidence": 0.9968976974487305, "text_region": [[155.0, 1053.0], [558.0, 1053.0], [558.0, 1075.0], [155.0, 1075.0]]}, {"text": "to go through this chaotic mix to", "confidence": 0.9990480542182922, "text_region": [[151.0, 1085.0], [558.0, 1084.0], [558.0, 1112.0], [151.0, 1114.0]]}, {"text": "pick details that are essential to", "confidence": 0.9921853542327881, "text_region": [[151.0, 1118.0], [553.0, 1116.0], [553.0, 1145.0], [151.0, 1146.0]]}, {"text": "answering the query accurately. As", "confidence": 0.9991211891174316, "text_region": [[155.0, 1153.0], [593.0, 1153.0], [593.0, 1177.0], [155.0, 1177.0]]}, {"text": "you can imagine, using a simple", "confidence": 0.9981389045715332, "text_region": [[155.0, 1185.0], [557.0, 1185.0], [557.0, 1209.0], [155.0, 1209.0]]}, {"text": "prompt or a chain of thought.", "confidence": 0.9622732400894165, "text_region": [[153.0, 1216.0], [521.0, 1214.0], [521.0, 1243.0], [153.0, 1245.0]]}, {"text": "prompting method can do little to", "confidence": 0.995303750038147, "text_region": [[153.0, 1248.0], [577.0, 1244.0], [577.0, 1273.0], [153.0, 1277.0]]}, {"text": "instruct the model to pick the right", "confidence": 0.9933057427406311, "text_region": [[150.0, 1278.0], [584.0, 1280.0], [584.0, 1309.0], [150.0, 1307.0]]}, {"text": "details.", "confidence": 0.9995356798171997, "text_region": [[153.0, 1314.0], [242.0, 1314.0], [242.0, 1338.0], [153.0, 1338.0]]}, {"text": "The idea behind ThoT prompting is", "confidence": 0.9986101984977722, "text_region": [[151.0, 1375.0], [586.0, 1377.0], [586.0, 1406.0], [151.0, 1404.0]]}, {"text": "instructing the model to:", "confidence": 0.9998000860214233, "text_region": [[150.0, 1409.0], [461.0, 1411.0], [460.0, 1439.0], [150.0, 1438.0]]}, {"text": "Go step-by-step", "confidence": 0.9996640682220459, "text_region": [[193.0, 1473.0], [411.0, 1477.0], [410.0, 1506.0], [192.0, 1502.0]]}, {"text": " Summarize each step", "confidence": 0.9365086555480957, "text_region": [[160.0, 1529.0], [473.0, 1531.0], [472.0, 1560.0], [160.0, 1558.0]]}, {"text": ". Analyze each step", "confidence": 0.9494249224662781, "text_region": [[155.0, 1583.0], [428.0, 1587.0], [428.0, 1616.0], [155.0, 1612.0]]}, {"text": "Galileo", "confidence": 0.9954356551170349, "text_region": [[244.0, 1997.0], [351.0, 1997.0], [351.0, 2032.0], [244.0, 2032.0]]}, {"text": "www.rungalileo.io", "confidence": 0.9990150332450867, "text_region": [[1149.0, 2003.0], [1336.0, 2003.0], [1336.0, 2027.0], [1149.0, 2027.0]]}], "img_idx": 0}
