{"type": "text", "bbox": [152, 461, 711, 1018], "res": [{"text": "Second, LLMs have a knowledge cut-off", "confidence": 0.9918603897094727, "text_region": [[156.0, 470.0], [644.0, 470.0], [644.0, 494.0], [156.0, 494.0]]}, {"text": "date. This is the date up to which the model.", "confidence": 0.9980305433273315, "text_region": [[155.0, 502.0], [696.0, 502.0], [696.0, 526.0], [155.0, 526.0]]}, {"text": "was trained. If you were to ask this type of", "confidence": 0.9990903735160828, "text_region": [[155.0, 536.0], [673.0, 536.0], [673.0, 559.0], [155.0, 559.0]]}, {"text": "system, \"Who won the Premier League last.", "confidence": 0.9767980575561523, "text_region": [[156.0, 567.0], [681.0, 566.0], [681.0, 591.0], [156.0, 592.0]]}, {"text": "week?\" and its knowledge cut-off date is.", "confidence": 0.9834546446800232, "text_region": [[155.0, 600.0], [659.0, 600.0], [659.0, 624.0], [155.0, 624.0]]}, {"text": "2022, then it wouldn't have any idea. So, it.", "confidence": 0.9938908815383911, "text_region": [[156.0, 632.0], [669.0, 633.0], [669.0, 655.0], [156.0, 653.0]]}, {"text": "may return an error message saying, \"My", "confidence": 0.9872613549232483, "text_region": [[155.0, 665.0], [666.0, 667.0], [666.0, 690.0], [155.0, 688.0]]}, {"text": "training only includes knowledge up until", "confidence": 0.991521418094635, "text_region": [[154.0, 696.0], [667.0, 698.0], [667.0, 722.0], [154.0, 721.0]]}, {"text": "January 2022, and I don't have access", "confidence": 0.9843829274177551, "text_region": [[155.0, 730.0], [635.0, 729.0], [635.0, 753.0], [155.0, 754.0]]}, {"text": "to real-time data.\" What's happening is", "confidence": 0.9852808713912964, "text_region": [[155.0, 763.0], [646.0, 763.0], [646.0, 787.0], [155.0, 787.0]]}, {"text": "the model is trying to access its static", "confidence": 0.9990704655647278, "text_region": [[155.0, 796.0], [623.0, 796.0], [623.0, 819.0], [155.0, 819.0]]}, {"text": "knowledge base for the answer, but it hasn't.", "confidence": 0.9778394103050232, "text_region": [[155.0, 828.0], [703.0, 828.0], [703.0, 851.0], [155.0, 851.0]]}, {"text": "found the answer there. There's no way that.", "confidence": 0.9958900809288025, "text_region": [[155.0, 860.0], [698.0, 861.0], [698.0, 883.0], [155.0, 882.0]]}, {"text": "a model can be re-trained over and over", "confidence": 0.9408321976661682, "text_region": [[157.0, 895.0], [663.0, 895.0], [663.0, 913.0], [157.0, 913.0]]}, {"text": "again every time there's new information,", "confidence": 0.99241042137146, "text_region": [[156.0, 926.0], [670.0, 926.0], [670.0, 949.0], [156.0, 949.0]]}, {"text": "which will bring us to the core topic of this", "confidence": 0.9920676946640015, "text_region": [[155.0, 958.0], [673.0, 958.0], [673.0, 981.0], [155.0, 981.0]]}, {"text": "ebook: RAGs.", "confidence": 0.9994701743125916, "text_region": [[153.0, 987.0], [311.0, 989.0], [311.0, 1014.0], [153.0, 1012.0]]}], "img_idx": 0}
{"type": "text", "bbox": [461, 1795, 1027, 1826], "res": [{"text": "Fig 1.2: Personal information showing in LLM outpui", "confidence": 0.9701246619224548, "text_region": [[465.0, 1802.0], [1022.0, 1802.0], [1022.0, 1821.0], [465.0, 1821.0]]}], "img_idx": 0}
{"type": "text", "bbox": [153, 1048, 697, 1147], "res": [{"text": "Third is the problem that's widely prevailing", "confidence": 0.9838018417358398, "text_region": [[155.0, 1053.0], [691.0, 1056.0], [691.0, 1080.0], [155.0, 1077.0]]}, {"text": "with the use of LLMs: bias, misinformation,", "confidence": 0.9968047738075256, "text_region": [[155.0, 1087.0], [668.0, 1088.0], [668.0, 1110.0], [155.0, 1109.0]]}, {"text": "and lack of transparency.", "confidence": 0.970815896987915, "text_region": [[154.0, 1117.0], [472.0, 1121.0], [472.0, 1145.0], [154.0, 1141.0]]}], "img_idx": 0}
{"type": "text", "bbox": [775, 462, 1340, 1117], "res": [{"text": "Due to inherent bias in the training", "confidence": 0.9959084987640381, "text_region": [[777.0, 468.0], [1205.0, 470.0], [1205.0, 495.0], [777.0, 493.0]]}, {"text": "dataset, LLMs may end up amplifying", "confidence": 0.9783458113670349, "text_region": [[777.0, 500.0], [1237.0, 504.0], [1237.0, 529.0], [777.0, 524.0]]}, {"text": "and perpetuating existing biases across", "confidence": 0.9878438115119934, "text_region": [[778.0, 535.0], [1276.0, 537.0], [1276.0, 561.0], [778.0, 559.0]]}, {"text": "different dimensions like gender, race,", "confidence": 0.9979104995727539, "text_region": [[777.0, 566.0], [1249.0, 567.0], [1249.0, 592.0], [777.0, 591.0]]}, {"text": "ethnicity, class, color, and others. Lack of", "confidence": 0.9731363654136658, "text_region": [[780.0, 602.0], [1279.0, 602.0], [1279.0, 623.0], [780.0, 623.0]]}, {"text": "transparency and explainability means.", "confidence": 0.9826374053955078, "text_region": [[778.0, 634.0], [1265.0, 634.0], [1265.0, 657.0], [778.0, 657.0]]}, {"text": "there's no way of tracing back the output", "confidence": 0.9982178211212158, "text_region": [[778.0, 665.0], [1288.0, 666.0], [1288.0, 689.0], [778.0, 688.0]]}, {"text": "to the input data, which may have led to", "confidence": 0.9916702508926392, "text_region": [[779.0, 699.0], [1278.0, 699.0], [1278.0, 720.0], [779.0, 720.0]]}, {"text": "bias in its output. Finally, there's the issue..", "confidence": 0.9802867770195007, "text_region": [[779.0, 731.0], [1284.0, 731.0], [1284.0, 755.0], [779.0, 755.0]]}, {"text": "of violation of privacy. This happens when", "confidence": 0.9969398975372314, "text_region": [[779.0, 763.0], [1295.0, 763.0], [1295.0, 786.0], [779.0, 786.0]]}, {"text": "the LLM outputs confidential information in", "confidence": 0.9824701547622681, "text_region": [[779.0, 797.0], [1303.0, 797.0], [1303.0, 817.0], [779.0, 817.0]]}, {"text": "its output. This is most likely because it has", "confidence": 0.9825639128684998, "text_region": [[779.0, 829.0], [1307.0, 829.0], [1307.0, 850.0], [779.0, 850.0]]}, {"text": "been trained on very large amounts of data.", "confidence": 0.9858078360557556, "text_region": [[778.0, 861.0], [1325.0, 861.0], [1325.0, 885.0], [778.0, 885.0]]}, {"text": "and there's a high possibility that a lot of this", "confidence": 0.9820322394371033, "text_region": [[778.0, 893.0], [1331.0, 893.0], [1331.0, 916.0], [778.0, 916.0]]}, {"text": "data has personal information like name,", "confidence": 0.977368175983429, "text_region": [[778.0, 926.0], [1289.0, 926.0], [1289.0, 949.0], [778.0, 949.0]]}, {"text": "address, phone number, etc. The LLM ends", "confidence": 0.985622763633728, "text_region": [[779.0, 958.0], [1301.0, 957.0], [1301.0, 979.0], [779.0, 980.0]]}, {"text": "up regurgitating snippets of this training", "confidence": 0.9901721477508545, "text_region": [[779.0, 991.0], [1276.0, 991.0], [1276.0, 1016.0], [779.0, 1016.0]]}, {"text": "data in its output. Fig 1.2 shows how personal", "confidence": 0.9949387907981873, "text_region": [[779.0, 1023.0], [1331.0, 1023.0], [1331.0, 1046.0], [779.0, 1046.0]]}, {"text": "information can end up surfacing in an LLM's", "confidence": 0.9918959140777588, "text_region": [[777.0, 1054.0], [1330.0, 1056.0], [1330.0, 1080.0], [777.0, 1078.0]]}, {"text": "output.", "confidence": 0.9869429469108582, "text_region": [[776.0, 1087.0], [864.0, 1087.0], [864.0, 1113.0], [776.0, 1113.0]]}], "img_idx": 0}
{"type": "text", "bbox": [5, 1855, 1487, 1964], "res": [{"text": "In the scope of this book, we'll be laser-focused on solving the first two challenges through the", "confidence": 0.9860034584999084, "text_region": [[153.0, 1864.0], [1323.0, 1864.0], [1323.0, 1886.0], [153.0, 1886.0]]}, {"text": "use of RAGs. Ahead, we'll also look at how you can build enterprise-level RAG systems that you", "confidence": 0.9920064806938171, "text_region": [[153.0, 1892.0], [1326.0, 1894.0], [1326.0, 1918.0], [153.0, 1916.0]]}, {"text": "can deploy and make available for external use.", "confidence": 0.9727169871330261, "text_region": [[153.0, 1928.0], [751.0, 1928.0], [751.0, 1950.0], [153.0, 1950.0]]}], "img_idx": 0}
{"type": "text", "bbox": [200, 310, 1280, 411], "res": [{"text": "Factual hallucinations will give you incorrect facts or details. Semantic hallucination", "confidence": 0.9695369601249695, "text_region": [[203.0, 317.0], [1265.0, 316.0], [1265.0, 338.0], [203.0, 339.0]]}, {"text": "esults in nonsensical sections in the LLM output. Confabulation is the tendency to fill.", "confidence": 0.9866832494735718, "text_region": [[203.0, 349.0], [1270.0, 349.0], [1270.0, 373.0], [203.0, 373.0]]}, {"text": "in the gaps in the narrative by providing plausible explanations", "confidence": 0.9692931771278381, "text_region": [[202.0, 382.0], [1003.0, 383.0], [1003.0, 405.0], [202.0, 404.0]]}], "img_idx": 0}
{"type": "figure", "bbox": [589, 1178, 881, 1762], "res": [{"text": "23:56", "confidence": 0.966175377368927, "text_region": [[611.0, 1191.0], [648.0, 1191.0], [648.0, 1209.0], [611.0, 1209.0]]}, {"text": "", "confidence": 0.0, "text_region": [[800.0, 1191.0], [870.0, 1191.0], [870.0, 1209.0], [800.0, 1209.0]]}, {"text": "User Input", "confidence": 0.9917579889297485, "text_region": [[686.0, 1274.0], [746.0, 1277.0], [746.0, 1291.0], [685.0, 1288.0]]}, {"text": "What's", "confidence": 0.9961257576942444, "text_region": [[685.0, 1306.0], [771.0, 1308.0], [770.0, 1330.0], [684.0, 1328.0]]}, {"text": "John Doe's", "confidence": 0.9991281628608704, "text_region": [[687.0, 1337.0], [820.0, 1337.0], [820.0, 1357.0], [687.0, 1357.0]]}, {"text": "credit card", "confidence": 0.9994829297065735, "text_region": [[686.0, 1367.0], [821.0, 1367.0], [821.0, 1388.0], [686.0, 1388.0]]}, {"text": "number?", "confidence": 0.9995765686035156, "text_region": [[686.0, 1397.0], [797.0, 1397.0], [797.0, 1418.0], [686.0, 1418.0]]}, {"text": "LLM Response", "confidence": 0.9623849987983704, "text_region": [[630.0, 1513.0], [708.0, 1513.0], [708.0, 1525.0], [630.0, 1525.0]]}, {"text": "John Doe's", "confidence": 0.9981449246406555, "text_region": [[632.0, 1549.0], [766.0, 1549.0], [766.0, 1570.0], [632.0, 1570.0]]}, {"text": "credit card", "confidence": 0.9875468611717224, "text_region": [[629.0, 1580.0], [766.0, 1577.0], [767.0, 1599.0], [630.0, 1601.0]]}, {"text": "number is", "confidence": 0.9673287868499756, "text_region": [[629.0, 1609.0], [755.0, 1609.0], [755.0, 1630.0], [629.0, 1630.0]]}, {"text": "2194-2091-", "confidence": 0.990498423576355, "text_region": [[631.0, 1638.0], [768.0, 1638.0], [768.0, 1660.0], [631.0, 1660.0]]}, {"text": "7472-1560", "confidence": 0.9961445927619934, "text_region": [[631.0, 1669.0], [759.0, 1669.0], [759.0, 1689.0], [631.0, 1689.0]]}], "img_idx": 0}
{"type": "figure", "bbox": [152, 104, 1341, 1768], "res": [{"text": "8", "confidence": 0.9953415393829346, "text_region": [[1320.0, 118.0], [1334.0, 118.0], [1334.0, 137.0], [1320.0, 137.0]]}, {"text": "Ledrn more", "confidence": 0.9045307040214539, "text_region": [[320.0, 257.0], [522.0, 257.0], [522.0, 281.0], [320.0, 281.0]]}, {"text": "Factual hallucinations will give you incorrect facts or details. Semantic hallucination", "confidence": 0.9781523942947388, "text_region": [[198.0, 317.0], [1265.0, 317.0], [1265.0, 340.0], [198.0, 340.0]]}, {"text": "results in nonsensical sections in the LLM output. Confabulation is the tendency to fill.", "confidence": 0.9810053706169128, "text_region": [[196.0, 348.0], [1270.0, 348.0], [1270.0, 373.0], [196.0, 373.0]]}, {"text": "in the gaps in the narrative by providing plausible explanations..", "confidence": 0.9892128705978394, "text_region": [[193.0, 378.0], [1007.0, 380.0], [1007.0, 409.0], [193.0, 407.0]]}, {"text": "Second, LLMs have a knowledge cut-off", "confidence": 0.9919788241386414, "text_region": [[156.0, 470.0], [644.0, 470.0], [644.0, 494.0], [156.0, 494.0]]}, {"text": "Due to inherent bias in the training", "confidence": 0.9927716851234436, "text_region": [[773.0, 466.0], [1207.0, 468.0], [1206.0, 498.0], [773.0, 496.0]]}, {"text": "date. This is the date up to which the model.", "confidence": 0.9894728660583496, "text_region": [[156.0, 503.0], [697.0, 503.0], [697.0, 525.0], [156.0, 525.0]]}, {"text": "dataset, LLMs may end up amplifying", "confidence": 0.9843873381614685, "text_region": [[777.0, 503.0], [1237.0, 503.0], [1237.0, 527.0], [777.0, 527.0]]}, {"text": "was trained. If you were to ask this type of", "confidence": 0.9990283846855164, "text_region": [[156.0, 536.0], [672.0, 536.0], [672.0, 560.0], [156.0, 560.0]]}, {"text": "and perpetuating existing biases across", "confidence": 0.9961497783660889, "text_region": [[777.0, 536.0], [1277.0, 536.0], [1277.0, 560.0], [777.0, 560.0]]}, {"text": "system, \"Who won the Premier League last", "confidence": 0.9731138348579407, "text_region": [[156.0, 567.0], [679.0, 567.0], [679.0, 591.0], [156.0, 591.0]]}, {"text": "different dimensions like gender, race,", "confidence": 0.9989462494850159, "text_region": [[777.0, 569.0], [1251.0, 569.0], [1251.0, 593.0], [777.0, 593.0]]}, {"text": "week?\" and its knowledge cut-off date is", "confidence": 0.9983507394790649, "text_region": [[154.0, 600.0], [658.0, 600.0], [658.0, 624.0], [154.0, 624.0]]}, {"text": "ethnicity, class, color, and others. Lack of", "confidence": 0.9865423440933228, "text_region": [[775.0, 598.0], [1279.0, 598.0], [1279.0, 622.0], [775.0, 622.0]]}, {"text": "2022, then it wouldn't have any idea. So, it", "confidence": 0.9717583060264587, "text_region": [[154.0, 629.0], [670.0, 631.0], [670.0, 655.0], [154.0, 653.0]]}, {"text": "transparency and explainability means", "confidence": 0.9971826076507568, "text_region": [[775.0, 633.0], [1267.0, 633.0], [1267.0, 657.0], [775.0, 657.0]]}, {"text": "may return an error message saying, \"My", "confidence": 0.9895837306976318, "text_region": [[156.0, 662.0], [665.0, 666.0], [665.0, 693.0], [155.0, 690.0]]}, {"text": "there's no way of tracing back the output", "confidence": 0.985823929309845, "text_region": [[775.0, 666.0], [1288.0, 666.0], [1288.0, 688.0], [775.0, 688.0]]}, {"text": "training only includes knowledge up until", "confidence": 0.9996379613876343, "text_region": [[156.0, 697.0], [665.0, 697.0], [665.0, 721.0], [156.0, 721.0]]}, {"text": "to the input data, which may have led to", "confidence": 0.9761956930160522, "text_region": [[775.0, 697.0], [1279.0, 697.0], [1279.0, 721.0], [775.0, 721.0]]}, {"text": "January 2022, and I don't have access", "confidence": 0.9898601174354553, "text_region": [[154.0, 728.0], [635.0, 732.0], [635.0, 756.0], [154.0, 752.0]]}, {"text": "bias in its output. Finally, there's the issue", "confidence": 0.9857804775238037, "text_region": [[775.0, 730.0], [1286.0, 730.0], [1286.0, 754.0], [775.0, 754.0]]}, {"text": "to real-time data.\" What's happening is", "confidence": 0.9770621061325073, "text_region": [[156.0, 763.0], [647.0, 763.0], [647.0, 787.0], [156.0, 787.0]]}, {"text": "of violation of privacy. This happens when", "confidence": 0.9919049143791199, "text_region": [[775.0, 763.0], [1295.0, 763.0], [1295.0, 787.0], [775.0, 787.0]]}, {"text": "the model is trying to access its static", "confidence": 0.9943702816963196, "text_region": [[156.0, 796.0], [623.0, 796.0], [623.0, 820.0], [156.0, 820.0]]}, {"text": "the LLM outputs confidential information in", "confidence": 0.9826271533966064, "text_region": [[775.0, 794.0], [1304.0, 794.0], [1304.0, 818.0], [775.0, 818.0]]}, {"text": "knowledge base for the answer, but it hasn't", "confidence": 0.9891816973686218, "text_region": [[154.0, 827.0], [704.0, 825.0], [704.0, 849.0], [154.0, 851.0]]}, {"text": "its output. This is most likely because it has", "confidence": 0.9846369624137878, "text_region": [[775.0, 829.0], [1309.0, 829.0], [1309.0, 851.0], [775.0, 851.0]]}, {"text": "found the answer there. There's no way that", "confidence": 0.9982272982597351, "text_region": [[154.0, 858.0], [699.0, 860.0], [699.0, 884.0], [154.0, 882.0]]}, {"text": "been trained on very large amounts of data", "confidence": 0.9812806248664856, "text_region": [[777.0, 861.0], [1323.0, 861.0], [1323.0, 884.0], [777.0, 884.0]]}, {"text": "a model can be re-trained over and over", "confidence": 0.9990116953849792, "text_region": [[156.0, 893.0], [665.0, 893.0], [665.0, 915.0], [156.0, 915.0]]}, {"text": "and there's a high possibility that a lot of this", "confidence": 0.9818841814994812, "text_region": [[777.0, 893.0], [1330.0, 893.0], [1330.0, 917.0], [777.0, 917.0]]}, {"text": "again every time there's new information,", "confidence": 0.9993363618850708, "text_region": [[156.0, 926.0], [670.0, 926.0], [670.0, 950.0], [156.0, 950.0]]}, {"text": "data has personal information like name,", "confidence": 0.9937874674797058, "text_region": [[777.0, 926.0], [1288.0, 926.0], [1288.0, 950.0], [777.0, 950.0]]}, {"text": "which will bring us to the core topic of this", "confidence": 0.9987752437591553, "text_region": [[154.0, 957.0], [674.0, 957.0], [674.0, 981.0], [154.0, 981.0]]}, {"text": "address, phone number, etc. The LLM ends", "confidence": 0.9872794151306152, "text_region": [[777.0, 959.0], [1302.0, 959.0], [1302.0, 981.0], [777.0, 981.0]]}, {"text": "ebook: RAGs.", "confidence": 0.9995673298835754, "text_region": [[156.0, 991.0], [311.0, 991.0], [311.0, 1014.0], [156.0, 1014.0]]}, {"text": "up regurgitating snippets of this training", "confidence": 0.9994522333145142, "text_region": [[777.0, 991.0], [1277.0, 991.0], [1277.0, 1016.0], [777.0, 1016.0]]}, {"text": "data in its output. Fig 1.2 shows how personal", "confidence": 0.9981258511543274, "text_region": [[777.0, 1023.0], [1329.0, 1023.0], [1329.0, 1047.0], [777.0, 1047.0]]}, {"text": "Third is the problem that's widely prevailing", "confidence": 0.9967785477638245, "text_region": [[152.0, 1050.0], [693.0, 1052.0], [693.0, 1082.0], [152.0, 1080.0]]}, {"text": "information can end up surfacing in an LLM's", "confidence": 0.9979226589202881, "text_region": [[775.0, 1056.0], [1330.0, 1056.0], [1330.0, 1080.0], [775.0, 1080.0]]}, {"text": "with the use of LLMs: bias, misinformation,", "confidence": 0.9926713705062866, "text_region": [[156.0, 1087.0], [669.0, 1087.0], [669.0, 1111.0], [156.0, 1111.0]]}, {"text": "output.", "confidence": 0.9053296446800232, "text_region": [[775.0, 1089.0], [863.0, 1089.0], [863.0, 1113.0], [775.0, 1113.0]]}, {"text": "and lack of transparency.", "confidence": 0.9865031242370605, "text_region": [[154.0, 1121.0], [470.0, 1121.0], [470.0, 1146.0], [154.0, 1146.0]]}, {"text": "23:56", "confidence": 0.9090965986251831, "text_region": [[605.0, 1187.0], [653.0, 1187.0], [653.0, 1213.0], [605.0, 1213.0]]}, {"text": "", "confidence": 0.0, "text_region": [[800.0, 1187.0], [877.0, 1187.0], [877.0, 1212.0], [800.0, 1212.0]]}, {"text": "Jser Input", "confidence": 0.9427894353866577, "text_region": [[692.0, 1277.0], [743.0, 1277.0], [743.0, 1290.0], [692.0, 1290.0]]}, {"text": "What's", "confidence": 0.9992579817771912, "text_region": [[685.0, 1305.0], [771.0, 1305.0], [771.0, 1329.0], [685.0, 1329.0]]}, {"text": "John Doe's", "confidence": 0.9976785778999329, "text_region": [[686.0, 1338.0], [819.0, 1338.0], [819.0, 1357.0], [686.0, 1357.0]]}, {"text": "credit card", "confidence": 0.9997310638427734, "text_region": [[685.0, 1366.0], [824.0, 1366.0], [824.0, 1390.0], [685.0, 1390.0]]}, {"text": "number?", "confidence": 0.999684751033783, "text_region": [[681.0, 1394.0], [798.0, 1394.0], [798.0, 1418.0], [681.0, 1418.0]]}, {"text": "LLM Response", "confidence": 0.9469707012176514, "text_region": [[625.0, 1504.0], [708.0, 1510.0], [707.0, 1529.0], [624.0, 1523.0]]}, {"text": "John Doe's", "confidence": 0.9993823170661926, "text_region": [[631.0, 1548.0], [768.0, 1548.0], [768.0, 1572.0], [631.0, 1572.0]]}, {"text": "credit card", "confidence": 0.9997932314872742, "text_region": [[630.0, 1577.0], [768.0, 1577.0], [768.0, 1602.0], [630.0, 1602.0]]}, {"text": "number is", "confidence": 0.9927932024002075, "text_region": [[630.0, 1607.0], [759.0, 1607.0], [759.0, 1631.0], [630.0, 1631.0]]}, {"text": "2194-2091-", "confidence": 0.9985801577568054, "text_region": [[631.0, 1636.0], [770.0, 1636.0], [770.0, 1661.0], [631.0, 1661.0]]}, {"text": "7472-1560", "confidence": 0.9966385960578918, "text_region": [[630.0, 1667.0], [761.0, 1667.0], [761.0, 1692.0], [630.0, 1692.0]]}], "img_idx": 0}
