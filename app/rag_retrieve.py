from chromadb import PersistentClient
from sentence_transformers import SentenceTransformer
from transformers import pipeline

# === Initialize embeddings & ChromaDB ===
embedder = SentenceTransformer("BAAI/bge-small-en")
client = PersistentClient(path="vectorstore")

try:
    collection = client.get_collection("rag_docs")
except:
    collection = client.get_collection("rag_chunks")

print(f"üìö Using collection: {collection.name}")

# === Initialize LLM ===
print("üß† Loading local LLM model (flan-t5-base)...")
generator = pipeline("text2text-generation", model="google/flan-t5-base")

# === Query Loop ===
while True:
    query = input("\n‚ùì Enter your question (or type 'exit' to quit): ")
    if query.lower() in ["exit", "quit"]:
        print("üëã Exiting RAG Assistant.")
        break

    # 1Ô∏è‚É£ Retrieve top chunks
    query_emb = embedder.encode([query]).tolist()
    results = collection.query(query_embeddings=query_emb, n_results=5)
    docs = results.get("documents", [[]])[0]

    if not docs:
        print("‚ö†Ô∏è No relevant information found.")
        continue

    # 2Ô∏è‚É£ Combine top contexts
    context = "\n\n".join(docs)

    # 3Ô∏è‚É£ Generate answer using LLM
    prompt = f"Answer the question based on the context below.\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:"
    answer = generator(prompt, max_new_tokens=200, temperature=0.7)[0]["generated_text"]

    # 4Ô∏è‚É£ Print results
    print("\nüí¨ RAG Answer:\n", answer)
